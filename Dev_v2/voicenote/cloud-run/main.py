import os
import logging
import asyncio
from contextlib import asynccontextmanager
from typing import Dict, Any, Optional

from fastapi import FastAPI, HTTPException, BackgroundTasks, Request
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
import uvicorn

# Google Cloud
from google.cloud import firestore, storage, logging as cloud_logging

# pyannote.audioèªè¨¼è¨­å®š
import torch
from huggingface_hub import login
from dotenv import load_dotenv

# è‡ªä½œãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«
from audio_processor import AudioProcessor
from speaker_separation import SpeakerSeparationService
from transcription_apis import TranscriptionService, APIConfig
from voice_learning import VoiceLearningService

# ç’°å¢ƒå¤‰æ•°èª­ã¿è¾¼ã¿
load_dotenv()

def setup_pyannote_authentication():
    """pyannote.audioèªè¨¼è¨­å®š"""
    try:
        hf_token = os.getenv('HUGGINGFACE_TOKEN')
        if hf_token:
            login(token=hf_token)
            logger.info("âœ… Hugging Face authentication successful")
        else:
            logger.warning("âš ï¸ HUGGINGFACE_TOKEN not found - some models may not be accessible")
    except Exception as e:
        logger.error(f"âŒ Hugging Face authentication failed: {e}")

def setup_pytorch_settings():
    """PyTorchè¨­å®šæœ€é©åŒ–"""
    try:
        # CPUã®ã¿ã§ã®å‹•ä½œã‚’è¨­å®šï¼ˆCloud Runç’°å¢ƒï¼‰
        torch.set_num_threads(4)  # Cloud Runã®4vCPUã«æœ€é©åŒ–
        
        # ãƒ¡ãƒ¢ãƒªåŠ¹ç‡åŒ–
        if torch.cuda.is_available():
            logger.info("ğŸš€ CUDA available - GPU acceleration enabled")
        else:
            logger.info("ğŸ’» Using CPU for audio processing")
            
        logger.info("âœ… PyTorch settings configured")
    except Exception as e:
        logger.error(f"âŒ PyTorch setup failed: {e}")

# ãƒ­ã‚°è¨­å®š
if os.getenv('GOOGLE_CLOUD_PROJECT'):
    # Cloud Runã§ã®å®Ÿè¡Œæ™‚
    client = cloud_logging.Client()
    client.setup_logging()

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# èªè¨¼ãƒ»è¨­å®šåˆæœŸåŒ–
setup_pyannote_authentication()
setup_pytorch_settings()

# Firebase ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆåˆæœŸåŒ–
db = firestore.Client()
storage_client = storage.Client()

# ã‚µãƒ¼ãƒ“ã‚¹åˆæœŸåŒ–
audio_processor = AudioProcessor()
speaker_service = SpeakerSeparationService()
transcription_service = TranscriptionService()
voice_learning_service = VoiceLearningService()

@asynccontextmanager
async def lifespan(app: FastAPI):
    """ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ãƒ©ã‚¤ãƒ•ã‚µã‚¤ã‚¯ãƒ«ç®¡ç†"""
    logger.info("ğŸš€ VoiceNote Audio Processing Service starting...")
    
    # åˆæœŸåŒ–å‡¦ç†
    try:
        await audio_processor.initialize()
        logger.info("âœ… Audio processor initialized")
    except Exception as e:
        logger.error(f"âŒ Failed to initialize audio processor: {e}")
    
    yield
    
    # ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—å‡¦ç†
    logger.info("ğŸ”„ Shutting down VoiceNote Audio Processing Service...")

# FastAPI ã‚¢ãƒ—ãƒªä½œæˆ
app = FastAPI(
    title="VoiceNote Audio Processing Service",
    description="é«˜ç²¾åº¦éŸ³å£°æ–‡å­—èµ·ã“ã—ãƒ»è©±è€…åˆ†é›¢ã‚µãƒ¼ãƒ“ã‚¹",
    version="1.0.0",
    lifespan=lifespan
)

# CORSè¨­å®š
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # æœ¬ç•ªç’°å¢ƒã§ã¯é©åˆ‡ãªã‚ªãƒªã‚¸ãƒ³ã‚’è¨­å®š
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# ãƒªã‚¯ã‚¨ã‚¹ãƒˆãƒ¢ãƒ‡ãƒ«
class ProcessAudioRequest(BaseModel):
    user_id: str
    audio_id: str
    config: Dict[str, Any] = {}

class VoiceLearningRequest(BaseModel):
    user_id: str
    audio_data: str  # Base64ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ã•ã‚ŒãŸéŸ³å£°ãƒ‡ãƒ¼ã‚¿
    session_id: str

class TranscriptionRequest(BaseModel):
    user_id: str
    audio_path: str
    api_config: Dict[str, Any]
    segments: Optional[list] = None

# ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯
@app.get("/health")
async def health_check():
    """ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ"""
    return {
        "status": "healthy",
        "service": "voicenote-audio-processor",
        "version": "1.0.0"
    }

# éŸ³å£°å‡¦ç†é–‹å§‹
@app.post("/process-audio")
async def process_audio(request: ProcessAudioRequest, background_tasks: BackgroundTasks):
    """éŸ³å£°å‡¦ç†é–‹å§‹ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ"""
    try:
        logger.info(f"Starting audio processing for user {request.user_id}, audio {request.audio_id}")
        
        # ãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰ã‚¿ã‚¹ã‚¯ã¨ã—ã¦å‡¦ç†ã‚’é–‹å§‹
        background_tasks.add_task(
            run_audio_processing,
            request.user_id,
            request.audio_id,
            request.config
        )
        
        return {
            "status": "processing_started",
            "message": f"éŸ³å£°å‡¦ç†ã‚’é–‹å§‹ã—ã¾ã—ãŸ: {request.audio_id}",
            "user_id": request.user_id,
            "audio_id": request.audio_id
        }
        
    except Exception as e:
        logger.error(f"Failed to start audio processing: {e}")
        raise HTTPException(status_code=500, detail=str(e))

async def run_audio_processing(user_id: str, audio_id: str, config: Dict[str, Any]):
    """éŸ³å£°å‡¦ç†å®Ÿè¡Œï¼ˆãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰ï¼‰"""
    try:
        logger.info(f"Running audio processing: {user_id}/{audio_id}")
        
        # å‡¦ç†è¨­å®šï¼ˆAPIè¨­å®šã‚’å«ã‚€ï¼‰
        processing_config = {
            "enable_speaker_separation": config.get("enable_speaker_separation", True),
            "max_speakers": config.get("max_speakers", 5),
            "use_user_embedding": config.get("use_user_embedding", True),
            "language": config.get("language", "ja"),
            "chunk_duration": config.get("chunk_duration", 30),
            "overlap_duration": config.get("overlap_duration", 5),
            # APIè¨­å®šã‚’è¿½åŠ 
            "transcription_config": {
                "provider": config.get("speech_provider", "openai"),
                "api_key": config.get("speech_api_key", ""),
                "model": config.get("speech_model", "whisper-1"),
                "settings": config.get("speech_settings", {})
            },
            "llm_config": {
                "provider": config.get("llm_provider", "openai"),
                "api_key": config.get("llm_api_key", ""),
                "model": config.get("llm_model", "gpt-4"),
                "settings": config.get("llm_settings", {})
            }
        }
        
        logger.info(f"Processing config: {dict(processing_config, transcription_config={'provider': processing_config['transcription_config']['provider'], 'api_key': '***masked***'}, llm_config={'provider': processing_config['llm_config']['provider'], 'api_key': '***masked***'})}")
        
        # éŸ³å£°å‡¦ç†å®Ÿè¡Œ
        result = await audio_processor.process_audio(
            user_id=user_id,
            audio_id=audio_id,
            config=processing_config
        )
        
        logger.info(f"Audio processing completed: {user_id}/{audio_id}")
        
        # å‡¦ç†å®Œäº†ã‚’Firestoreã«è¨˜éŒ²
        await update_audio_status(user_id, audio_id, "completed", 100, result)
        
    except Exception as e:
        logger.error(f"Audio processing failed: {user_id}/{audio_id} - {e}")
        
        # ã‚¨ãƒ©ãƒ¼ã‚’Firestoreã«è¨˜éŒ²
        await update_audio_status(user_id, audio_id, "error", 0, {"error": str(e)})

# è©±è€…åˆ†é›¢
@app.post("/speaker-separation")
async def speaker_separation(request: ProcessAudioRequest):
    """è©±è€…åˆ†é›¢ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ"""
    try:
        # éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã®å–å¾—
        audio_path = await download_audio_file(request.user_id, request.audio_id)
        
        # ãƒ¦ãƒ¼ã‚¶ãƒ¼åŸ‹ã‚è¾¼ã¿ã®å–å¾—
        user_embedding = await get_user_embedding(request.user_id)
        
        # è©±è€…åˆ†é›¢å®Ÿè¡Œ
        if request.config.get("use_chunking", False):
            # ãƒãƒ£ãƒ³ã‚¯åˆ†å‰²å‡¦ç†
            chunks = await split_audio_to_chunks(audio_path, request.config)
            result = await speaker_service.analyze_speakers_chunked(
                chunks, 
                user_embedding=user_embedding
            )
        else:
            # ç›´æ¥å‡¦ç†
            result = await speaker_service.analyze_speakers(
                audio_path,
                max_speakers=request.config.get("max_speakers", 5),
                user_embedding=user_embedding
            )
        
        return {
            "status": "success",
            "result": result
        }
        
    except Exception as e:
        logger.error(f"Speaker separation failed: {e}")
        raise HTTPException(status_code=500, detail=str(e))

# æ–‡å­—èµ·ã“ã—
@app.post("/transcription")
async def transcription(request: TranscriptionRequest):
    """æ–‡å­—èµ·ã“ã—ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ"""
    try:
        # APIè¨­å®šä½œæˆ
        api_config = APIConfig(
            provider=request.api_config["provider"],
            api_key=request.api_config["api_key"],
            model=request.api_config["model"],
            language=request.api_config.get("language", "ja-JP"),
            settings=request.api_config.get("settings", {})
        )
        
        if request.segments:
            # ã‚»ã‚°ãƒ¡ãƒ³ãƒˆæ¯ã®æ–‡å­—èµ·ã“ã—
            results = await transcription_service.transcribe_segments_batch(
                request.audio_path,
                request.segments,
                api_config
            )
        else:
            # å…¨ä½“æ–‡å­—èµ·ã“ã—
            api_client = transcription_service.create_api_client(api_config)
            result = await api_client.transcribe(request.audio_path)
            results = [result]
        
        return {
            "status": "success",
            "results": [
                {
                    "text": r.text,
                    "confidence": r.confidence,
                    "segments": r.segments,
                    "language": r.language,
                    "processing_time": r.processing_time,
                    "provider": r.provider,
                    "model": r.model
                }
                for r in results
            ]
        }
        
    except Exception as e:
        logger.error(f"Transcription failed: {e}")
        raise HTTPException(status_code=500, detail=str(e))

# éŸ³å£°å­¦ç¿’
@app.post("/voice-learning")
async def voice_learning(request: VoiceLearningRequest):
    """éŸ³å£°å­¦ç¿’ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ"""
    try:
        result = await voice_learning_service.process_learning_audio(
            user_id=request.user_id,
            audio_data=request.audio_data,
            session_id=request.session_id
        )
        
        return {
            "status": "success",
            "result": result
        }
        
    except Exception as e:
        logger.error(f"Voice learning failed: {e}")
        raise HTTPException(status_code=500, detail=str(e))

# å‡¦ç†ã‚­ãƒ£ãƒ³ã‚»ãƒ«
@app.post("/cancel-processing")
async def cancel_processing(request: ProcessAudioRequest):
    """éŸ³å£°å‡¦ç†ã‚­ãƒ£ãƒ³ã‚»ãƒ«"""
    try:
        await update_audio_status(
            request.user_id, 
            request.audio_id, 
            "cancelled", 
            0, 
            {"cancelled_at": firestore.SERVER_TIMESTAMP}
        )
        
        return {
            "status": "cancelled",
            "message": "å‡¦ç†ã‚’ã‚­ãƒ£ãƒ³ã‚»ãƒ«ã—ã¾ã—ãŸ"
        }
        
    except Exception as e:
        logger.error(f"Failed to cancel processing: {e}")
        raise HTTPException(status_code=500, detail=str(e))

# å‡¦ç†çŠ¶æ³ç¢ºèª
@app.get("/processing-status/{user_id}/{audio_id}")
async def get_processing_status(user_id: str, audio_id: str):
    """å‡¦ç†çŠ¶æ³ç¢ºèª"""
    try:
        doc_ref = db.collection('audios').document(user_id).collection('files').document(audio_id)
        doc = doc_ref.get()
        
        if not doc.exists:
            raise HTTPException(status_code=404, detail="Audio file not found")
        
        data = doc.to_dict()
        
        return {
            "status": data.get("status", "unknown"),
            "progress": data.get("processingProgress", 0),
            "message": data.get("statusMessage", ""),
            "current_chunk": data.get("processedChunks"),
            "total_chunks": data.get("totalChunks"),
            "updated_at": data.get("updatedAt")
        }
        
    except Exception as e:
        logger.error(f"Failed to get processing status: {e}")
        raise HTTPException(status_code=500, detail=str(e))

# ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£é–¢æ•°
async def update_audio_status(user_id: str, audio_id: str, status: str, 
                            progress: int, additional_data: Dict[str, Any] = None):
    """éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã®çŠ¶æ…‹æ›´æ–°"""
    try:
        doc_ref = db.collection('audios').document(user_id).collection('files').document(audio_id)
        
        update_data = {
            'status': status,
            'processingProgress': progress,
            'updatedAt': firestore.SERVER_TIMESTAMP
        }
        
        if additional_data:
            update_data.update(additional_data)
        
        doc_ref.update(update_data)
        
    except Exception as e:
        logger.error(f"Failed to update audio status: {e}")
        raise

async def download_audio_file(user_id: str, audio_id: str) -> str:
    """Cloud Storageã‹ã‚‰éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰"""
    # å®Ÿè£…äºˆå®š
    pass

async def get_user_embedding(user_id: str) -> Optional[list]:
    """ãƒ¦ãƒ¼ã‚¶ãƒ¼éŸ³å£°åŸ‹ã‚è¾¼ã¿ã‚’å–å¾—"""
    try:
        doc_ref = db.collection('userEmbeddings').document(user_id)
        doc = doc_ref.get()
        
        if doc.exists:
            data = doc.to_dict()
            return data.get('embedding')
        
        return None
        
    except Exception as e:
        logger.error(f"Failed to get user embedding: {e}")
        return None

async def split_audio_to_chunks(audio_path: str, config: Dict[str, Any]) -> list:
    """éŸ³å£°ã‚’ãƒãƒ£ãƒ³ã‚¯ã«åˆ†å‰²"""
    # å®Ÿè£…äºˆå®š
    pass

# ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œ
if __name__ == "__main__":
    port = int(os.environ.get("PORT", 8080))
    
    uvicorn.run(
        "main:app",
        host="0.0.0.0",
        port=port,
        log_level="info",
        workers=1  # Cloud Runã§ã¯1ãƒ¯ãƒ¼ã‚«ãƒ¼æ¨å¥¨
    )