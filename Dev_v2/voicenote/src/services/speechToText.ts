import { AudioFile, TranscriptionSegment, Transcription } from '@/types';

// éŸ³å£°èªè­˜ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã®è¨­å®š
export interface SpeechToTextConfig {
  provider: 'openai' | 'azure' | 'google' | 'assembly' | 'deepgram';
  apiKey: string;
  model?: string;
  language?: string;
}

// éŸ³å£°èªè­˜çµæœ
export interface SpeechToTextResult {
  text: string;
  segments: TranscriptionSegment[];
  speakers: string[];
  confidence: number;
  processingTime: number;
  language: string;
}

export class SpeechToTextService {
  private static instance: SpeechToTextService;

  static getInstance(): SpeechToTextService {
    if (!SpeechToTextService.instance) {
      SpeechToTextService.instance = new SpeechToTextService();
    }
    return SpeechToTextService.instance;
  }

  // ãƒ‡ãƒ¢ç”¨ã®æ–‡å­—èµ·ã“ã—çµæœç”Ÿæˆ
  private generateDemoTranscription(audioFile: AudioFile): SpeechToTextResult {
    console.log('ğŸ­ SpeechToText: Generating demo transcription for:', audioFile.fileName);
    
    const demoSegments: TranscriptionSegment[] = [
      {
        start: 0,
        end: 5,
        text: 'ã“ã‚“ã«ã¡ã¯ã€ä»Šæ—¥ã¯ãŠå¿™ã—ã„ä¸­ãŠæ™‚é–“ã‚’ã„ãŸã ãã‚ã‚ŠãŒã¨ã†ã”ã–ã„ã¾ã™ã€‚',
        speaker: 'ã‚ãªãŸ',
        confidence: 0.95
      },
      {
        start: 5,
        end: 12,
        text: 'ã“ã¡ã‚‰ã“ãã€ã‚ˆã‚ã—ããŠé¡˜ã„ã—ã¾ã™ã€‚æ—©é€Ÿã§ã™ãŒã€ä»Šå›ã®ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã«ã¤ã„ã¦èª¬æ˜ã•ã›ã¦ã„ãŸã ãã¾ã™ã€‚',
        speaker: 'Aã•ã‚“',
        confidence: 0.92
      },
      {
        start: 12,
        end: 18,
        text: 'ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®æ¦‚è¦ã«ã¤ã„ã¦ã¯ç†è§£ã—ã¾ã—ãŸãŒã€ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«ã¯ã©ã®ã‚ˆã†ã«ãªã£ã¦ã„ã¾ã™ã§ã—ã‚‡ã†ã‹ï¼Ÿ',
        speaker: 'Bã•ã‚“',
        confidence: 0.88
      },
      {
        start: 18,
        end: 25,
        text: 'ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«ã«ã¤ã„ã¦ã¯ã€æ¥æœˆã®ç¬¬ä¸€é€±ã‹ã‚‰é–‹å§‹äºˆå®šã§ã™ã€‚è©³ç´°ãªã‚¿ã‚¤ãƒ ãƒ©ã‚¤ãƒ³ã‚’ãŠé€ã‚Šã—ã¾ã™ã€‚',
        speaker: 'ã‚ãªãŸ',
        confidence: 0.94
      },
      {
        start: 25,
        end: 32,
        text: 'äºˆç®—ã«ã¤ã„ã¦ã¯ã„ã‹ãŒã§ã—ã‚‡ã†ã‹ã€‚å‰å›ã®ææ¡ˆã‹ã‚‰å¤‰æ›´ã¯ã‚ã‚Šã¾ã™ã‹ï¼Ÿ',
        speaker: 'Bã•ã‚“',
        confidence: 0.90
      },
      {
        start: 32,
        end: 40,
        text: 'äºˆç®—ã«ã¤ã„ã¦ã¯å‰å›ææ¡ˆã‹ã‚‰20%å‰Šæ¸›ã™ã‚‹ã“ã¨ã§èª¿æ•´ã„ãŸã—ã¾ã™ã€‚å“è³ªã¯ç¶­æŒã—ãŸã¾ã¾åŠ¹ç‡åŒ–ã‚’å›³ã‚Šã¾ã™ã€‚',
        speaker: 'Aã•ã‚“',
        confidence: 0.93
      }
    ];

    const fullText = demoSegments.map(segment => segment.text).join(' ');
    const speakers = ['ã‚ãªãŸ', 'Aã•ã‚“', 'Bã•ã‚“'];

    return {
      text: fullText,
      segments: demoSegments,
      speakers,
      confidence: 0.92,
      processingTime: Math.floor(audioFile.duration * 0.2), // éŸ³å£°é•·ã®20%ã®å‡¦ç†æ™‚é–“
      language: 'ja'
    };
  }

  // OpenAI Whisper API ã§ã®æ–‡å­—èµ·ã“ã—
  private async transcribeWithOpenAI(
    audioUrl: string, 
    config: SpeechToTextConfig
  ): Promise<SpeechToTextResult> {
    try {
      // å®Ÿéš›ã®OpenAI APIå‘¼ã³å‡ºã—ã¯ã“ã“ã«å®Ÿè£…
      // ãƒ‡ãƒ¢ãƒ¢ãƒ¼ãƒ‰ã§ã¯æ¨¡æ“¬çµæœã‚’è¿”ã™
      throw new Error('OpenAI API implementation needed');
    } catch (error) {
      console.error('OpenAI transcription failed:', error);
      throw error;
    }
  }

  // Azure Speech Services ã§ã®æ–‡å­—èµ·ã“ã—
  private async transcribeWithAzure(
    audioUrl: string, 
    config: SpeechToTextConfig
  ): Promise<SpeechToTextResult> {
    try {
      // å®Ÿéš›ã®Azure APIå‘¼ã³å‡ºã—ã¯ã“ã“ã«å®Ÿè£…
      throw new Error('Azure API implementation needed');
    } catch (error) {
      console.error('Azure transcription failed:', error);
      throw error;
    }
  }

  // Google Cloud Speech ã§ã®æ–‡å­—èµ·ã“ã—
  private async transcribeWithGoogle(
    audioUrl: string, 
    config: SpeechToTextConfig
  ): Promise<SpeechToTextResult> {
    try {
      // å®Ÿéš›ã®Google APIå‘¼ã³å‡ºã—ã¯ã“ã“ã«å®Ÿè£…
      throw new Error('Google API implementation needed');
    } catch (error) {
      console.error('Google transcription failed:', error);
      throw error;
    }
  }

  // Assembly AI ã§ã®æ–‡å­—èµ·ã“ã—
  private async transcribeWithAssembly(
    audioUrl: string, 
    config: SpeechToTextConfig
  ): Promise<SpeechToTextResult> {
    try {
      // å®Ÿéš›ã®Assembly AI APIå‘¼ã³å‡ºã—ã¯ã“ã“ã«å®Ÿè£…
      throw new Error('Assembly AI implementation needed');
    } catch (error) {
      console.error('Assembly AI transcription failed:', error);
      throw error;
    }
  }

  // Deepgram ã§ã®æ–‡å­—èµ·ã“ã—
  private async transcribeWithDeepgram(
    audioUrl: string, 
    config: SpeechToTextConfig
  ): Promise<SpeechToTextResult> {
    try {
      // å®Ÿéš›ã®Deepgram APIå‘¼ã³å‡ºã—ã¯ã“ã“ã«å®Ÿè£…
      throw new Error('Deepgram API implementation needed');
    } catch (error) {
      console.error('Deepgram transcription failed:', error);
      throw error;
    }
  }

  // ãƒ¡ã‚¤ãƒ³æ–‡å­—èµ·ã“ã—é–¢æ•°
  async transcribeAudio(
    audioFile: AudioFile, 
    config?: SpeechToTextConfig
  ): Promise<Transcription> {
    console.log('ğŸ—£ï¸ Starting transcription for:', audioFile.fileName);
    
    // ãƒ‡ãƒ¢ãƒ¢ãƒ¼ãƒ‰ã¾ãŸã¯è¨­å®šãŒãªã„å ´åˆ
    if (audioFile.userId === 'demo-user-123' || !config) {
      console.log('ğŸ­ Using demo transcription');
      const demoResult = this.generateDemoTranscription(audioFile);
      
      return {
        text: demoResult.text,
        segments: demoResult.segments,
        speakers: demoResult.speakers,
        language: demoResult.language,
        confidence: demoResult.confidence,
        processingTime: demoResult.processingTime,
        apiProvider: 'demo',
        model: 'demo-whisper'
      };
    }

    const startTime = Date.now();
    let result: SpeechToTextResult;

    try {
      // ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã«å¿œã˜ã¦é©åˆ‡ãªAPIã‚’å‘¼ã³å‡ºã—
      switch (config.provider) {
        case 'openai':
          result = await this.transcribeWithOpenAI(audioFile.fileUrl, config);
          break;
        case 'azure':
          result = await this.transcribeWithAzure(audioFile.fileUrl, config);
          break;
        case 'google':
          result = await this.transcribeWithGoogle(audioFile.fileUrl, config);
          break;
        case 'assembly':
          result = await this.transcribeWithAssembly(audioFile.fileUrl, config);
          break;
        case 'deepgram':
          result = await this.transcribeWithDeepgram(audioFile.fileUrl, config);
          break;
        default:
          throw new Error(`Unsupported provider: ${config.provider}`);
      }

      const processingTime = Date.now() - startTime;

      return {
        text: result.text,
        segments: result.segments,
        speakers: result.speakers,
        language: result.language,
        confidence: result.confidence,
        processingTime: processingTime,
        apiProvider: config.provider,
        model: config.model || 'default'
      };

    } catch (error) {
      console.error('âŒ Transcription failed:', error);
      
      // ã‚¨ãƒ©ãƒ¼æ™‚ã¯ãƒ‡ãƒ¢ãƒ‡ãƒ¼ã‚¿ã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
      console.log('ğŸ”„ Falling back to demo transcription');
      const demoResult = this.generateDemoTranscription(audioFile);
      
      return {
        text: demoResult.text,
        segments: demoResult.segments,
        speakers: demoResult.speakers,
        language: demoResult.language,
        confidence: demoResult.confidence,
        processingTime: Date.now() - startTime,
        apiProvider: 'demo-fallback',
        model: 'demo-whisper'
      };
    }
  }

  // è©±è€…åˆ†é›¢å‡¦ç†ï¼ˆãƒ‡ãƒ¢å®Ÿè£…ï¼‰
  async performSpeakerDiarization(audioFile: AudioFile): Promise<{
    speakers: string[];
    segments: TranscriptionSegment[];
  }> {
    console.log('ğŸ­ Performing demo speaker diarization for:', audioFile.fileName);
    
    // ãƒ‡ãƒ¢ç”¨ã®è©±è€…åˆ†é›¢çµæœ
    const speakers = ['ã‚ãªãŸ', 'Aã•ã‚“', 'Bã•ã‚“'];
    const segments: TranscriptionSegment[] = [
      { start: 0, end: 5, text: '', speaker: 'ã‚ãªãŸ', confidence: 0.95 },
      { start: 5, end: 12, text: '', speaker: 'Aã•ã‚“', confidence: 0.92 },
      { start: 12, end: 18, text: '', speaker: 'Bã•ã‚“', confidence: 0.88 },
      { start: 18, end: 25, text: '', speaker: 'ã‚ãªãŸ', confidence: 0.94 },
      { start: 25, end: 32, text: '', speaker: 'Bã•ã‚“', confidence: 0.90 },
      { start: 32, end: 40, text: '', speaker: 'Aã•ã‚“', confidence: 0.93 }
    ];

    return { speakers, segments };
  }

  // éŸ³å£°å‡¦ç†ã®é€²æ—æ›´æ–°
  async updateProcessingProgress(
    audioId: string, 
    userId: string, 
    progress: number, 
    status: string
  ): Promise<void> {
    console.log(`ğŸ“Š Updating progress for ${audioId}: ${progress}% - ${status}`);
    
    // å®Ÿéš›ã®å®Ÿè£…ã§ã¯Firestoreã‚’æ›´æ–°
    // await databaseService.updateAudioFile(userId, audioId, { 
    //   processingProgress: progress,
    //   status 
    // });
  }
}

export const speechToTextService = SpeechToTextService.getInstance();